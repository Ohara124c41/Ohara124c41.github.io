---
layout: post
title: "Unsupervised Learning for Software Defect Prediction"
subtitle: "KC1 (NASA), Method Motivations, Effort‚ÄìDifficulty Maps, and PHM Transfer"
author: "Christopher O‚ÄôHara"
header-style: text
tags:
  - Unsupervised
  - PHM
  - Fault Detection
  - Anomaly Prediction
  - Software Metrics
  - SOM
  - Autoencoder
---

Modern engineering often begins with **lots of data and few trustworthy labels**. The NASA **KC1** software dataset is a classic example: many module metrics (Halstead, complexity, coupling), sparse/uncertain defect labels, and strong class imbalance. In such settings, unsupervised learning isn‚Äôt a convenience‚Äîit‚Äôs a **design requirement**: we need to *discover structure*, *measure difficulty/risk*, and *prioritize work* **before** labels are reliable.

Machine learning, especially in unsupervised settings, rarely gives us immediate satisfaction. Many outputs look messy or unhelpful at first. But each one carries meaning, and together they build the case for a final actionable result. Below, I reflect on the KC1 images and explain why frustration is part of the process‚Äîand why trusting the stepwise theory matters.

This post explains **why** each method was chosen, **what** it reveals that others do not, **how** the results translate into a practical **effort‚Äìdifficulty** map for prioritizing code review and testing, and **where** these ideas transfer to **PHM** (prognostics & health management) and anomaly detection in physical systems.

üëâ Full code and experiments: **[KAS NASA KC1 Notebook](https://github.com/Ohara124c41/Unsupervised_Learning_KC1/blob/main/KAS_NASA_kc1.ipynb)**

---

## Problem Framing: Why Unsupervised First?

- **Reality**: defect labels are expensive, noisy, and imbalanced (~few defects among many modules).  
- **Goal**: *Triage* modules by **difficulty/risk** and allocate limited engineering **effort** (review, testing, refactor) for best ROI.  
- **Principle**: start with **structure discovery** (manifolds, clusters, outliers) ‚Üí then **risk scoring** ‚Üí finally **prioritization** using an **effort‚Äìdifficulty** lens.

We use Halstead **effort** ($e$) and **volume** ($v$) as effort proxies, while **difficulty** comes from unsupervised **risk signals** (cluster separation, anomaly scores, reconstruction error, SOM activations).

---

## Data Preprocessing (Motivation)

**Why normalize/denoise first?**  
KC1 metrics are heterogeneous (orders of magnitude apart); unscaled distances distort neighborhood and density estimates, which **breaks** clustering and anomaly detection. PCA/t-SNE are only meaningful when features are on comparable scales.

**What we do**  
- Standardize features; winsorize extreme tails if necessary.  
- Handle missing values; remove trivially collinear metrics.  
- Preserve $e$ and $v$ for later **effort mapping**.

---

## PCA ‚Äî Linear Structure, Denoising, and Interpretable Axes

**Motivation.**  
If variation is approximately linear, **PCA** gives the **least-squares optimal** low-dimensional basis. It reduces noise, mitigates collinearity, and yields axes engineers understand (e.g., *size/volume* vs *complexity/coupling*). It is the **baseline manifold** for everything that follows.

**What PCA reveals that others don‚Äôt.**  
- Global variance directions (not just local neighborhoods).  
- How much of the dataset geometry is *explainable linearly*.  
- A physically interpretable 2D plane to overlay effort/volume.

**Limitations.**  
Misses curved manifolds; clusters can appear merged.

**Figure ‚Äî PCA**  
![KC1 PCA](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_PCA.png?raw=true)

At first glance, this figure is frustrating. The clusters look arbitrary‚Äîone large dense blob and some scattered points far away. It doesn‚Äôt scream ‚Äúdefects‚Äù or ‚Äúeffort‚Äìdifficulty.‚Äù But if we stop here, we‚Äôd miss the point: clustering in raw PCA space mostly tells us that *linear variance alone doesn‚Äôt cleanly separate risk*.  

**Interpretation:** There are **different regimes of code modules**, but their boundaries aren‚Äôt sharply aligned with defects. It‚Äôs not satisfying, but it shows us why we need richer models.

---

## t-SNE ‚Äî Local Neighborhoods and Nonlinear Manifolds

**Motivation.**  
When defect patterns are **nonlinear** or locally tight, **t-SNE** preserves **local neighborhoods** better than PCA. It‚Äôs a **visual microscope** for seeing pockets of similar modules that may correspond to high-risk code styles.

**What t-SNE reveals.**  
- Fine-grained clumps that PCA smears.  
- Candidate cluster counts and unusual neighborhoods for inspection.

**Cautions.**  
Global distances are *not* meaningful; tune perplexity; fix random seeds for reproducibility; use as *exploration*, not a classifier.

## Clustering (k-Means / GMM / Hierarchical) ‚Äî From Structure to Groups

**Motivation.**  
Once we glimpse structure, we need **groups** to (1) summarize modules, (2) compare group-level risk, and (3) route different test strategies.

- **k-Means** (fast, centroid-based): good when clusters are roughly spherical in the feature space you use (PCA plane or standardized metrics).  
- **GMM** (soft probabilities): when clusters **overlap**; gives a **defect-likelihood proxy** via membership probabilities.  
- **Hierarchical**: exposes **multi-scale** structure; dendrogram helps choose $k$ data-driven rather than arbitrary.

**What clustering reveals.**  
- Stable mode families of modules (coding styles, complexity regimes).  
- Border cases with ambiguous membership ‚Üí **hardest to classify** ‚áí good candidates for review.

**Limitations.**  
Cluster shapes and scales matter; verify with silhouette/BIC; don‚Äôt assume one ‚Äútrue‚Äù $k$.

**Figure ‚Äî Clusters**  
![KC1 Clusters](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_Clusters.png?raw=true)

Here the picture looks noisy too: lots of overlapping clusters, spaghetti-like boundaries, and no obvious ‚Äúrisk frontier.‚Äù Frustrating? Yes. But the result is still interpretable: t-SNE is showing **local neighborhoods of similar modules**, and GMM probabilities are telling us that membership is fuzzy.  

**Interpretation:** This confirms that **defects aren‚Äôt globally distinct‚Äîthey live inside otherwise normal neighborhoods**. That‚Äôs valuable insight, even if the visual doesn‚Äôt ‚Äúsolve‚Äù the problem.

---

## Isolation Forest ‚Äî Density-Independent Outliers

**Motivation.**  
Clustering finds *centers*; we also need to find **rare events** that don‚Äôt belong anywhere. **Isolation Forest** isolates points by random splits; anomalies need **fewer splits**.

**What IF reveals.**  
- Local anomalies **within** otherwise ‚Äúsafe‚Äù clusters.  
- Outliers in **mixed metrics** where distance is unreliable.

**Limitations.**  
Sensitive to feature scaling and contamination rate; scores are relative, not absolute probabilities.

**Figure ‚Äî Isolation Forest**  
![KC1 Isolation Forest](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_IsolationForest.png?raw=true)

This output can also feel unrewarding: almost everything looks anomalous. Why bother? But the meaning here is that Isolation Forests **don‚Äôt guarantee balanced splits**‚Äîthey isolate data based on partition depth, and in skewed datasets like KC1, this leads to many ‚Äúoutliers.‚Äù  

**Interpretation:** *The method is telling us the dataset is extremely imbalanced, and defect-like behavior is scattered throughout.* Again, not a neat separation, but it teaches us that simple anomaly scores will over-flag without additional structure.

---

## Autoencoder ‚Äî Nonlinear Reconstruction Error as Difficulty

**Motivation.**  
When ‚Äúnormal‚Äù code lies on a **curved manifold**, the right notion of difficulty is **how far** a module is from what the network can reconstruct. Autoencoders supply a **nonlinear** anomaly score that complements IF.

**What AE reveals.**  
- Modules that are **manifold-distant**, even if not density-sparse.  
- Interactions among metrics (e.g., unusual coupling/size combos) invisible to linear models.

**Risk & mitigation.**  
If trained naively on mixed data, the AE can learn to reconstruct anomalies. Use early stopping, robust loss, or bias training to likely-normal regions.

**Figure ‚Äî Autoencoder**  
![KC1 Autoencoder](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_Autoencoder.png?raw=true)

The autoencoder output also feels disappointing at first: red and black points scattered without a crisp defect boundary. Where‚Äôs the signal? But what it‚Äôs really showing us is the **nonlinear reconstruction difficulty**: which modules deviate most from what the network considers ‚Äúnormal.‚Äù This is fair, because Professor [X] said AEs are good for every task! (small joke)

**Interpretation:** Even if it doesn‚Äôt form neat groups, it adds another *dimension of difficulty*. Modules with consistently high reconstruction error are telling us, ‚Äúthese combinations of metrics are unusual and fragile.‚Äù It‚Äôs not a final answer, but it adds another layer of evidence before we move to SOM.
---

## Self-Organizing Map (SOM) ‚Äî Topology + Operator Interpretability

**Motivation.**  
Engineers need **dashboards** where similar modules land in nearby cells. **SOMs** preserve topology on a 2D grid and let us **overlay task-specific fields** (effort, difficulty, delta). They are ideal for turning ML outputs into **actionable maps**.

**What SOM reveals.**  
- Regions of consistent coding style or complexity regime.  
- ‚ÄúHot‚Äù cells where many difficult modules accumulate.  
- A canvas to *combine* metrics and model scores.

**Limitations.**  
Grid size and initialization matter; treat as calibrated visualization rather than a classifier.

**Figure ‚Äî SOM overview**  
![KC1 SOM](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_SOM.png?raw=true)

---

## Results: Effort‚ÄìDifficulty Mapping (The ‚ÄúSo What‚Äù)

**Motivation.**  
Resources are limited. We must **prioritize** modules where improving quality is **cheapest for the largest risk reduction**. We operationalize:

- **Effort** ($E$): engineering cost proxy (Halstead $e$ or $v$, plus complexity/coupling as needed).  
- **Difficulty** ($D$): unsupervised risk score (e.g., max of normalized IF score, AE reconstruction error, GMM ‚Äúlow-likelihood‚Äù, cluster border proximity, SOM activation).

A simple, tunable priority score is  
$$
S = \lambda \, D + (1-\lambda)\, \tilde{E}
$$
with $\tilde{E}$ a scaled effort term. We then **plot on SOM** to get an operator-friendly triage map.

**Figure ‚Äî SOM effort‚Äìdifficulty**  

- **Difference map** (volume - difficulty): red = over-engineered/low risk (de-prioritize), blue = **low-effort/high-difficulty** (fast wins), mid-orange = **moderate-effort/high-difficulty** (best ROI):  
![KC1 SOM Diff](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_SOM_Diff.png?raw=true)

This is where things start to click. The SOM heatmaps finally tie the **effort proxy (volume)** to the **difficulty proxy (defect likelihood)** in a way that produces structure we can act on.  

- Some cells show high volume but low difficulty ‚Üí *large but stable code*.  
- Others show low volume but high difficulty ‚Üí *small, fragile modules*.  
- The difference map crystallizes the intuition: prioritize the **moderate-effort/high-difficulty zones** for best ROI.  

**Interpretation:** The SOM stage converts abstract metrics into an actionable **effort‚Äìdifficulty triage map**.

**Takeaways (KC1).**
1. Many **high-effort / low-difficulty** modules are false alarms‚Äîlarge but stable.  
2. A nontrivial pocket of **low-effort / high-difficulty** modules yields **quick wins**.  
3. The **moderate-effort / high-difficulty** band is the **sweet spot** for test/refactor investment.

This is the **operational output** of the project: a map that tells teams **where to spend the next hour**.

---

## How Methods Fit Together (Motivated Pipeline)

1. **PCA** ‚Üí robust, interpretable base space; remove noise/collinearity.  
2. **t-SNE** ‚Üí inspect nonlinear local structure; choose candidate $k$.  
3. **Clustering (k-Means/GMM/Hier.)** ‚Üí define groups & border cases; get soft difficulty via GMM likelihood.  
4. **Isolation Forest** ‚Üí catch density outliers that clusters miss.  
5. **Autoencoder** ‚Üí capture nonlinear reconstruction difficulty.  
6. **SOM** ‚Üí fuse $E$ & $D$ into **actionable 2D maps** for triage.

Each step is motivated by a **capability gap** left by the previous one.

### Results Journey ‚Äì Trusting the Process

The journey across these images is a reminder: in unsupervised ML, **each method reveals one lens**, often messy, rarely decisive.  

- PCA + k-Means ‚Üí showed us variance isn‚Äôt enough.  
- Isolation Forest ‚Üí exposed the imbalance problem.  
- t-SNE + GMM ‚Üí confirmed that defects live inside otherwise normal neighborhoods.  
- SOM ‚Üí finally delivered a clear prioritization map.  

If we judged PCA or IF results alone, we‚Äôd abandon the project in frustration. But step by step, layering clustering, anomaly detection, and topology-preserving maps, the landscape sharpened.  

**Lesson:** Not every figure must be satisfying, but we must **understand what each result means**‚Äîeven if it isn‚Äôt what we hoped for. Only then do we arrive at the actionable insight: the **effort‚Äìdifficulty map**.

---

## PHM / Fault Prediction Transfer (Why It Generalizes)


The broader systems design principles (see my [FSMs and Logic Controllers post](/2025/09/21/system_design.html)) apply here too:

- **Separation of concerns**: preprocessing (feature extraction) vs clustering (state discovery)  
- **Feedback and history**: anomaly detection improves when tracking data trajectories, not just snapshots  
- **Encoding and compositionality**: clusters and anomaly scores become **system codes** fed to schedulers, watchdogs, and diagnostics  

Replace ‚Äúmodule‚Äù with **component/sensor/subsystem**; replace $e,v$ with **maintenance effort proxies** (downtime, access cost, swap price). The same methods deliver:

- **PCA/t-SNE**: healthy manifold vs drifting states.  
- **Clustering**: operating modes and degradation stages.  
- **IF/AE**: rare precursors and nonlinear failure signatures.  
- **SOM**: control-room maps of **where** the plant/aircraft/robot is trending. 
- **Effort‚ÄìDifficulty**: plan inspections by **risk √ó cost**, not size or tradition.

---

## Why This Matters

Software defect prediction is structurally similar to **fault detection in engineered systems**:

- **Software modules** ‚Üí **mechanical subsystems, sensors, or valves**  
- **Defects** ‚Üí **faults, anomalies, or degradations**  
- **Code metrics** ‚Üí **sensor features, vibration signals, or process logs**  

Effort‚Äìdifficulty framing unifies these domains under a single principle:  
üëâ *Not all anomalies are equal. Prioritize based on both cost and likelihood.*


## Practical Notes and Caveats

- **Validation without labels**: use stability checks (seed/perplexity sweeps), cluster separation (silhouette), density diagnostics, and *post-hoc* audits on small labeled subsets.  
- **Thresholds**: treat anomaly thresholds as **operational dials** (precision/recall trade-offs differ for safety-critical vs exploratory review).  
- **Reproducibility**: fix seeds, log preprocessing; for t-SNE/AE, publish configs.  
- **Ethos**: unsupervised is *decision support*, not oracle‚Äîpair with code review and tests.


## Closing Thoughts

Unsupervised learning on KC1 highlights a pathway for **fault detection and PHM** in broader engineering contexts:

- **Software engineering** ‚Üí catching risky modules before deployment  
- **Aerospace** ‚Üí identifying precursors to subsystem failures  
- **Industry** ‚Üí clustering anomalies for predictive maintenance  
- **Robotics** ‚Üí embedding safety/awareness directly into navigation and control loops  

The lesson: **unsupervised structures exist everywhere**‚Äîby uncovering them, we enable systems to adapt, anticipate, and manage risk even when explicit guidance is missing.

---

## Conclusion

KC1 shows that **unsupervised learning is a systems tool**, not just a visualization trick. By **motivating** each method for what it uniquely contributes and **fusing** them on an interpretable **SOM effort‚Äìdifficulty map**, we move from ‚Äúinteresting plots‚Äù to **actionable triage**. The same blueprint scales to **PHM and anomaly detection** for physical systems where labels are scarce and stakes are high.

üëâ Notebook: **[KAS NASA KC1](https://github.com/Ohara124c41/Unsupervised_Learning_KC1/blob/main/KAS_NASA_kc1.ipynb)**  
