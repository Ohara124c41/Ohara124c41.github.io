---
layout: post
title: "Unsupervised Learning for Software Defect Prediction"
subtitle: "KC1 (NASA), Method Motivations, Effort‚ÄìDifficulty Maps, and PHM Transfer"
author: "Christopher O‚ÄôHara"
header-style: text
tags:
  - Unsupervised
  - PHM
  - Fault Detection
  - Anomaly Prediction
  - Software Metrics
  - SOM
  - Autoencoder
---

Modern engineering often begins with **lots of data and few trustworthy labels**. The NASA **KC1** software dataset is a classic example: many module metrics (Halstead, complexity, coupling), sparse/uncertain defect labels, and strong class imbalance. In such settings, unsupervised learning isn‚Äôt a convenience‚Äîit‚Äôs a **design requirement**: we need to *discover structure*, *measure difficulty/risk*, and *prioritize work* **before** labels are reliable.

This post explains **why** each method was chosen, **what** it reveals that others do not, **how** the results translate into a practical **effort‚Äìdifficulty** map for prioritizing code review and testing, and **where** these ideas transfer to **PHM** (prognostics & health management) and anomaly detection in physical systems.

üëâ Full code and experiments: **[KAS NASA KC1 Notebook](https://github.com/Ohara124c41/Unsupervised_Learning_KC1/blob/main/KAS_NASA_kc1.ipynb)**

---

## Problem Framing: Why Unsupervised First?

- **Reality**: defect labels are expensive, noisy, and imbalanced (~few defects among many modules).  
- **Goal**: *Triage* modules by **difficulty/risk** and allocate limited engineering **effort** (review, testing, refactor) for best ROI.  
- **Principle**: start with **structure discovery** (manifolds, clusters, outliers) ‚Üí then **risk scoring** ‚Üí finally **prioritization** using an **effort‚Äìdifficulty** lens.

We use Halstead **effort** ($e$) and **volume** ($v$) as effort proxies, while **difficulty** comes from unsupervised **risk signals** (cluster separation, anomaly scores, reconstruction error, SOM activations).

---

## Data Preprocessing (Motivation)

**Why normalize/denoise first?**  
KC1 metrics are heterogeneous (orders of magnitude apart); unscaled distances distort neighborhood and density estimates, which **breaks** clustering and anomaly detection. PCA/t-SNE are only meaningful when features are on comparable scales.

**What we do**  
- Standardize features; winsorize extreme tails if necessary.  
- Handle missing values; remove trivially collinear metrics.  
- Preserve $e$ and $v$ for later **effort mapping**.

---

## PCA ‚Äî Linear Structure, Denoising, and Interpretable Axes

**Motivation.**  
If variation is approximately linear, **PCA** gives the **least-squares optimal** low-dimensional basis. It reduces noise, mitigates collinearity, and yields axes engineers understand (e.g., *size/volume* vs *complexity/coupling*). It is the **baseline manifold** for everything that follows.

**What PCA reveals that others don‚Äôt.**  
- Global variance directions (not just local neighborhoods).  
- How much of the dataset geometry is *explainable linearly*.  
- A physically interpretable 2D plane to overlay effort/volume.

**Limitations.**  
Misses curved manifolds; clusters can appear merged.

**Figure ‚Äî PCA**  
![KC1 PCA](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_PCA.png?raw=true)

---

## t-SNE ‚Äî Local Neighborhoods and Nonlinear Manifolds

**Motivation.**  
When defect patterns are **nonlinear** or locally tight, **t-SNE** preserves **local neighborhoods** better than PCA. It‚Äôs a **visual microscope** for seeing pockets of similar modules that may correspond to high-risk code styles.

**What t-SNE reveals.**  
- Fine-grained clumps that PCA smears.  
- Candidate cluster counts and unusual neighborhoods for inspection.

**Cautions.**  
Global distances are *not* meaningful; tune perplexity; fix random seeds for reproducibility; use as *exploration*, not a classifier.

**Figure ‚Äî t-SNE**  
![KC1 t-SNE](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_tSNE.png?raw=true)

---

## Clustering (k-Means / GMM / Hierarchical) ‚Äî From Structure to Groups

**Motivation.**  
Once we glimpse structure, we need **groups** to (1) summarize modules, (2) compare group-level risk, and (3) route different test strategies.

- **k-Means** (fast, centroid-based): good when clusters are roughly spherical in the feature space you use (PCA plane or standardized metrics).  
- **GMM** (soft probabilities): when clusters **overlap**; gives a **defect-likelihood proxy** via membership probabilities.  
- **Hierarchical**: exposes **multi-scale** structure; dendrogram helps choose $k$ data-driven rather than arbitrary.

**What clustering reveals.**  
- Stable mode families of modules (coding styles, complexity regimes).  
- Border cases with ambiguous membership ‚Üí **hardest to classify** ‚áí good candidates for review.

**Limitations.**  
Cluster shapes and scales matter; verify with silhouette/BIC; don‚Äôt assume one ‚Äútrue‚Äù $k$.

**Figure ‚Äî Clusters**  
![KC1 Clusters](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_Clusters.png?raw=true)

---

## Isolation Forest ‚Äî Density-Independent Outliers

**Motivation.**  
Clustering finds *centers*; we also need to find **rare events** that don‚Äôt belong anywhere. **Isolation Forest** isolates points by random splits; anomalies need **fewer splits**.

**What IF reveals.**  
- Local anomalies **within** otherwise ‚Äúsafe‚Äù clusters.  
- Outliers in **mixed metrics** where distance is unreliable.

**Limitations.**  
Sensitive to feature scaling and contamination rate; scores are relative, not absolute probabilities.

**Figure ‚Äî Isolation Forest**  
![KC1 Isolation Forest](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_IsolationForest.png?raw=true)

---

## Autoencoder ‚Äî Nonlinear Reconstruction Error as Difficulty

**Motivation.**  
When ‚Äúnormal‚Äù code lies on a **curved manifold**, the right notion of difficulty is **how far** a module is from what the network can reconstruct. Autoencoders supply a **nonlinear** anomaly score that complements IF.

**What AE reveals.**  
- Modules that are **manifold-distant**, even if not density-sparse.  
- Interactions among metrics (e.g., unusual coupling/size combos) invisible to linear models.

**Risk & mitigation.**  
If trained naively on mixed data, the AE can learn to reconstruct anomalies. Use early stopping, robust loss, or bias training to likely-normal regions.

**Figure ‚Äî Autoencoder**  
![KC1 Autoencoder](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_Autoencoder.png?raw=true)

---

## Self-Organizing Map (SOM) ‚Äî Topology + Operator Interpretability

**Motivation.**  
Engineers need **dashboards** where similar modules land in nearby cells. **SOMs** preserve topology on a 2D grid and let us **overlay task-specific fields** (effort, difficulty, delta). They are ideal for turning ML outputs into **actionable maps**.

**What SOM reveals.**  
- Regions of consistent coding style or complexity regime.  
- ‚ÄúHot‚Äù cells where many difficult modules accumulate.  
- A canvas to *combine* metrics and model scores.

**Limitations.**  
Grid size and initialization matter; treat as calibrated visualization rather than a classifier.

**Figure ‚Äî SOM overview**  
![KC1 SOM](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_SOM.png?raw=true)

---

## Results: Effort‚ÄìDifficulty Mapping (The ‚ÄúSo What‚Äù)

**Motivation.**  
Resources are limited. We must **prioritize** modules where improving quality is **cheapest for the largest risk reduction**. We operationalize:

- **Effort** ($E$): engineering cost proxy (Halstead $e$ or $v$, plus complexity/coupling as needed).  
- **Difficulty** ($D$): unsupervised risk score (e.g., max of normalized IF score, AE reconstruction error, GMM ‚Äúlow-likelihood‚Äù, cluster border proximity, SOM activation).

A simple, tunable priority score is  
$$
S = \lambda \, D + (1-\lambda)\, \tilde{E}
$$
with $\tilde{E}$ a scaled effort term. We then **plot on SOM** to get an operator-friendly triage map.

**Figures ‚Äî SOM effort‚Äìdifficulty**  
- Side-by-side **effort vs difficulty** heatmaps on the SOM grid:  
![KC1 Effort Difficulty SOM](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_EffortDifficulty_SOM.png?raw=true)

- **Difference map** (volume - difficulty): red = over-engineered/low risk (de-prioritize), blue = **low-effort/high-difficulty** (fast wins), mid-orange = **moderate-effort/high-difficulty** (best ROI):  
![KC1 SOM Diff](https://github.com/Ohara124c41/Ohara124c41.github.io/blob/master/_posts/img/KC1_SOM_Diff.png?raw=true)

**Takeaways (KC1).**
1. Many **high-effort / low-difficulty** modules are false alarms‚Äîlarge but stable.  
2. A nontrivial pocket of **low-effort / high-difficulty** modules yields **quick wins**.  
3. The **moderate-effort / high-difficulty** band is the **sweet spot** for test/refactor investment.

This is the **operational output** of the project: a map that tells teams **where to spend the next hour**.

---

## How Methods Fit Together (Motivated Pipeline)

1. **PCA** ‚Üí robust, interpretable base space; remove noise/collinearity.  
2. **t-SNE** ‚Üí inspect nonlinear local structure; choose candidate $k$.  
3. **Clustering (k-Means/GMM/Hier.)** ‚Üí define groups & border cases; get soft difficulty via GMM likelihood.  
4. **Isolation Forest** ‚Üí catch density outliers that clusters miss.  
5. **Autoencoder** ‚Üí capture nonlinear reconstruction difficulty.  
6. **SOM** ‚Üí fuse $E$ & $D$ into **actionable 2D maps** for triage.

Each step is motivated by a **capability gap** left by the previous one.

---

## PHM / Fault Prediction Transfer (Why It Generalizes)

Replace ‚Äúmodule‚Äù with **component/sensor/subsystem**; replace $e,v$ with **maintenance effort proxies** (downtime, access cost, swap price). The same methods deliver:

- **PCA/t-SNE**: healthy manifold vs drifting states.  
- **Clustering**: operating modes and degradation stages.  
- **IF/AE**: rare precursors and nonlinear failure signatures.  
- **SOM**: control-room maps of **where** the plant/aircraft/robot is trending.  
- **Effort‚ÄìDifficulty**: plan inspections by **risk √ó cost**, not size or tradition.

---

## Practical Notes and Caveats

- **Validation without labels**: use stability checks (seed/perplexity sweeps), cluster separation (silhouette), density diagnostics, and *post-hoc* audits on small labeled subsets.  
- **Thresholds**: treat anomaly thresholds as **operational dials** (precision/recall trade-offs differ for safety-critical vs exploratory review).  
- **Reproducibility**: fix seeds, log preprocessing; for t-SNE/AE, publish configs.  
- **Ethos**: unsupervised is *decision support*, not oracle‚Äîpair with code review and tests.

---

## Conclusion

KC1 shows that **unsupervised learning is a systems tool**, not just a visualization trick. By **motivating** each method for what it uniquely contributes and **fusing** them on an interpretable **SOM effort‚Äìdifficulty map**, we move from ‚Äúinteresting plots‚Äù to **actionable triage**. The same blueprint scales to **PHM and anomaly detection** for physical systems where labels are scarce and stakes are high.

üëâ Notebook: **[KAS NASA KC1](https://github.com/Ohara124c41/Unsupervised_Learning_KC1/blob/main/KAS_NASA_kc1.ipynb)**  
